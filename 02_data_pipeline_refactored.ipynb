{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2. Refactored Data Cleaning Pipeline and Train-Test Loop\n",
    "### By Max Pechyonkin\n",
    "---\n",
    "\n",
    "## About Refactored Code\n",
    "\n",
    "This notebook uses exploratory information from the two previous notebooks and creates a single class `DataCleaner` to process assignment data. I did not explain why I made decisions to handle certain variables in a certain way because it was done in the previous notebooks.\n",
    "\n",
    "This class is optimized to process this particular dataset, with all its peculiarities and unique values.\n",
    "\n",
    "I thought about writing a more general class that would automatically detect inconsistencies and problems with data, but that would require having a clearly defined specification of the schema of the data with allowed values described, which I did not have. \n",
    "Another benefit of the `DataCleaner` is that it conforms to `sklearn`'s transformer protocol, which means it can be used as part of a `Pipeline` to simplify data processing and prediction pipelines. \n",
    "\n",
    "I was also planning to implement training and testing as a custom `Estimator` class, and then chain the `DataCleaner` and `Estimator` together in the `Pipeline` but due to time constraints that was not finished.\n",
    "\n",
    "**Note:** please also read the PDF notes acoompanying this submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from typing import Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series, Index\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import category_encoders as ce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner(TransformerMixin):\n",
    "    \"\"\"\n",
    "    This class performs all necessary cleaning required\n",
    "    to proceed to the modeling step:\n",
    "    1. remove all tows with NaNs\n",
    "    2. remove row with NaNs percentages > 99%\n",
    "    3. remove unnecessary columns:\n",
    "       - id-like columns to avoid overfitting\n",
    "       - columns with URLs\n",
    "       - columns with very messy unstructured data\n",
    "       - columns requiring too much work featurizing (like NLP)\n",
    "    4. perform cleaning on the remaining columns\n",
    "       - parse values with inconsistent formatting\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This data cleaner is stateless, but the method is\n",
    "        required by superclass.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        The data cleaner is stateless, but this method is required by superclass.\n",
    "        It does nothing in this case.\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"You don't need to fit for cleaning the data!\")\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_all_nans_columns(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove columns that have all NaNs\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: DataFrame with removed NaNs columns\n",
    "        \"\"\"\n",
    "\n",
    "        all_nan_cols = X.isnull().all()\n",
    "        all_nan_col_names = all_nan_cols[all_nan_cols].index\n",
    "        X = X.drop(all_nan_col_names, axis=1)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_nans_percentages(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Get percentages of NaNs for each column.\n",
    "\n",
    "        Returns Series with index of column names and float values.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: DataFrame with percentage of NaNs for each column of input DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        nrows, _ = X.shape\n",
    "        return (X.isnull().sum(axis=0) / nrows).sort_values(ascending=False)\n",
    "\n",
    "    def _get_colnames_with_nans_above_thresh(self, X: DataFrame, thresh: float) -> Index:\n",
    "        \"\"\"\n",
    "        Return column names with NaNs proportions above the given level.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :param thresh: threshold above which to remove columns\n",
    "        :return: Index with column names\n",
    "        \"\"\"\n",
    "\n",
    "        percentages = self._get_nans_percentages(X)\n",
    "        result = percentages[percentages > thresh]\n",
    "        return result.index\n",
    "\n",
    "    def _remove_cols_with_nans_above_thresh(self, X: DataFrame, thresh: float) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove columns that have NaNs percentage above a given threshold.\n",
    "        :param X: input DataFrame\n",
    "        :param thresh: threshold above which to remove columns\n",
    "        :return: DataFrame with removed columns\n",
    "        \"\"\"\n",
    "\n",
    "        colnames_with_too_many_nans = self._get_colnames_with_nans_above_thresh(X, thresh=thresh)\n",
    "        X = X.drop(colnames_with_too_many_nans, axis=1)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_unnecessary_columns(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove columns that I deemed unnecessary for one of three\n",
    "        reasons:\n",
    "          1. id-like data\n",
    "          2. too much effort needed to process column\n",
    "          3. data is too messy, noisy, unstructured\n",
    "          4. too many NaNs, and remaining non NaNs have too many unique values\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: DataFrame with columns removed\n",
    "        \"\"\"\n",
    "\n",
    "        cols_to_remove = [\n",
    "            'id',\n",
    "            'asins',\n",
    "            'dateAdded',\n",
    "            'dateUpdated',\n",
    "            'descriptions',\n",
    "            'ean',\n",
    "            'features',\n",
    "            'imageURLs',\n",
    "            'keys',\n",
    "            'manufacturerNumber',\n",
    "            'merchants',\n",
    "            'name',\n",
    "            'prices.amountMin',\n",
    "            'prices.dateAdded',\n",
    "            'prices.dateSeen',\n",
    "            'prices.sourceURLs',\n",
    "            'skus',\n",
    "            'sourceURLs',\n",
    "            'upc',\n",
    "            'weight',\n",
    "            'categories',\n",
    "            'prices.size',\n",
    "            'prices.color',\n",
    "        ]\n",
    "\n",
    "        X = X.drop(cols_to_remove, axis=1)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_currency_column(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove rows from df corresponding to currency column values that are not currencies.\n",
    "        Supported currencies are defined in the list below.\n",
    "\n",
    "        :param X: DataFrame with currencies column\n",
    "        :return: DataFrame with removed rows corresponding to illegal currencies.\n",
    "        \"\"\"\n",
    "\n",
    "        legal_currencies = [\n",
    "            'USD',\n",
    "            'AUD',\n",
    "            'CAD',\n",
    "            'EUR',\n",
    "            'GBP',\n",
    "        ]\n",
    "\n",
    "        rows_illegal_currency = X['prices.currency'].apply(lambda x: x not in legal_currencies)\n",
    "        rows_illegal_fx_idxs = rows_illegal_currency[rows_illegal_currency == True].index\n",
    "        X = X.drop(rows_illegal_fx_idxs, axis=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_invalid_price_string(s: str) -> bool:\n",
    "        \"\"\"\n",
    "        Return true if string represents a valid float number.\n",
    "\n",
    "        :param s: input string\n",
    "        :return: whether string can be converted to float\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: use try-except when casting to float to decide.\n",
    "        s = str(s)\n",
    "        illegal_chars = '-T:Z'\n",
    "        for c in illegal_chars:\n",
    "            if c in s:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _remove_illegal_price_values(self, X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove rows with illegal price values.\n",
    "\n",
    "        Note: this function doesn't perform conversion to float!\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: DataFrame with illegal rows removed\n",
    "        \"\"\"\n",
    "\n",
    "        illegal_price_mask = X['prices.amountMax'].apply(self._is_invalid_price_string)\n",
    "        illegal_price_idxs = illegal_price_mask[illegal_price_mask].index\n",
    "        X = X.drop(illegal_price_idxs)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_usd(row: Series, fx_rates: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Convert price from other currency to USD.\n",
    "\n",
    "        :param row: row from DataFrame to process\n",
    "        :param fx_rates: dictionary of fx rates\n",
    "        :return: price converted to USD\n",
    "        \"\"\"\n",
    "\n",
    "        price = float(row['prices.amountMax'])\n",
    "        currency = row['prices.currency']\n",
    "        if not currency:\n",
    "            # if currency is NaN assume USD and return price\n",
    "            return price\n",
    "        fx_rate = fx_rates[currency]\n",
    "        return price * fx_rate\n",
    "\n",
    "    def _convert_price_to_usd(self, X):\n",
    "        \"\"\"\n",
    "        Convert all prices to USD by using exchange rates.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: DataFrame with processed float price, converted to USD\n",
    "        \"\"\"\n",
    "\n",
    "        rates = {\n",
    "            'USD': 1.00,\n",
    "            'AUD': 0.68,\n",
    "            'CAD': 0.75,\n",
    "            'EUR': 1.10,\n",
    "            'GBP': 1.25,\n",
    "        }\n",
    "\n",
    "        X['price'] = X.apply(partial(self._convert_to_usd, fx_rates=rates), axis=1)\n",
    "        X = X.drop(['prices.amountMax'], axis=1)\n",
    "        return X\n",
    "\n",
    "    def _process_price_column(self, X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Three-step process of the price column:\n",
    "            1. process currency column\n",
    "            2. remove illegal price values\n",
    "            3. convert prices to USD\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._process_currency_column(X)\n",
    "        X = self._remove_illegal_price_values(X)\n",
    "        X = self._convert_price_to_usd(X)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_issale_column(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Process issale column, make sure it only has three values:\n",
    "          - True\n",
    "          - False\n",
    "          - NaN\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        def process_is_sale(x: Union[str, float]) -> Union[bool, float]:\n",
    "            \"\"\" Process individual entry in issale column. \"\"\"\n",
    "            if isinstance(x, bool):\n",
    "                return x\n",
    "            elif x.capitalize() == 'True':\n",
    "                return True\n",
    "            elif x.capitalize() == 'False':\n",
    "                return False\n",
    "            else:\n",
    "                print(x)\n",
    "                print(type(x))\n",
    "                raise ValueError(\"Something went wrong!\")\n",
    "\n",
    "        X['prices.isSale'] = X['prices.isSale'].apply(process_is_sale)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_colors_column(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Process colors: make them lowercase, remove spaces and dashes.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "        X['colors'] = X['colors'].apply(\n",
    "            lambda x: x.lower().replace(' ', '').replace('-', '') if not pd.isnull(x) else x)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_shipping_column(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Make shipping into two categories: free and not free.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        def process_shipping(x: Union[str, float]) -> Union[str, float]:\n",
    "            \"\"\" Process individual entry in shipping column. \"\"\"\n",
    "            if pd.isnull(x):\n",
    "                return x\n",
    "\n",
    "            x = x.lower()\n",
    "            if 'free' in x:\n",
    "                return 'free'\n",
    "            else:\n",
    "                return 'not free'\n",
    "\n",
    "        X['prices.shipping'] = X['prices.shipping'].apply(process_shipping)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_sizes_column(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate the number of sizes available.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        def process_sizes(x: Union[str, float]) -> Union[str, float]:\n",
    "            \"\"\" Process individual entry in sizes column. \"\"\"\n",
    "            if pd.isnull(x):\n",
    "                return x\n",
    "            return len(x.split(','))\n",
    "\n",
    "        X['sizes'] = X['sizes'].apply(process_sizes)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_dimension_column(X):\n",
    "        \"\"\"\n",
    "        Calculate total dimension: height + width + depth where available.\n",
    "\n",
    "        Also deals with 1 edge case of unique value.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        def process_dimensions(x: float) -> float:\n",
    "            \"\"\" Process individual entry in dimensions column. \"\"\"\n",
    "            if pd.isnull(x):\n",
    "                return x\n",
    "\n",
    "            # deal with edge case\n",
    "            if x == '32 inches':\n",
    "                return 32.0\n",
    "\n",
    "            x = x.lower()\n",
    "            dims = x.split('x')\n",
    "            dims = [d.replace(' ', '').replace('in', '') for d in dims]\n",
    "            dims = map(float, dims)\n",
    "            return sum(dims)\n",
    "\n",
    "        X['dimension'] = X['dimension'].apply(process_dimensions)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_review_ratings(X: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate average review rating, where available, otherwise set to zero.\n",
    "\n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        def parse_review(x: Union[str, float]) -> Union[str, float]:\n",
    "            \"\"\" Process individual entry in reviews column. \"\"\"\n",
    "            if pd.isnull(x):\n",
    "                return x\n",
    "\n",
    "            rating = 0.0\n",
    "            try:\n",
    "                reviews = json.loads(x)\n",
    "                for r in reviews:\n",
    "                    if 'rating' in r.keys():\n",
    "                        rating += float(r['rating'])\n",
    "                return rating / len(reviews)\n",
    "            except:\n",
    "                return rating\n",
    "\n",
    "        X['reviews'] = X['reviews'].apply(parse_review)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_price_outliers(X: DataFrame, thresh: float) -> DataFrame:\n",
    "\n",
    "        X = X.drop(X[X.price > thresh].index)\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_numerical_nans(X: DataFrame, value: float) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Convert NaNs of a numerical column to a given value, \n",
    "        this is required for categorical encoder later.\n",
    "        \n",
    "        :param X: input DataFrame\n",
    "        :return: processed DataFrame\n",
    "        \"\"\"\n",
    "        cols_to_fill_nas = ['dimension', 'reviews', 'sizes']\n",
    "        for c in cols_to_fill_nas:\n",
    "            X[c] = X[c].fillna(value)\n",
    "        return X\n",
    "\n",
    "    def transform(self, X: DataFrame):\n",
    "        \"\"\"\n",
    "        Process all the steps.\n",
    "\n",
    "        :param X: input uncleaned DataFrame\n",
    "        :return: cleaned and processed DataFrame, ready for modeling step\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._remove_all_nans_columns(X)\n",
    "        X = self._remove_cols_with_nans_above_thresh(X, thresh=0.99)\n",
    "        X = self._remove_unnecessary_columns(X)\n",
    "        X = self._process_price_column(X)\n",
    "        X = self._process_issale_column(X)\n",
    "        X = self._process_colors_column(X)\n",
    "        X = self._process_shipping_column(X)\n",
    "        X = self._process_sizes_column(X)\n",
    "        X = self._process_dimension_column(X)\n",
    "        X = self._calculate_review_ratings(X)\n",
    "        X = self._remove_price_outliers(X, thresh=2000)\n",
    "        X = self._convert_numerical_nans(X, value=-999.9)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, cleaning and splitting the data into features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '7004_1.csv'\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "data = pd.read_csv(DATA_PATH, error_bad_lines=False, warn_bad_lines=False)\n",
    "data = cleaner.transform(data)\n",
    "\n",
    "X, y = data.drop('price', axis=1), data['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regadring disctribution of the target\n",
    "\n",
    "Let's examine the distribution of the target. As we see, there is a long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logify(data):\n",
    "    \"\"\"\n",
    "    Applies log transform to make distribution more symmetrical.\n",
    "    Goes from price to log-price.\n",
    "    Note: 1 added for numerical stability (hadles zero input)\n",
    "    \"\"\"\n",
    "    return np.log(1 + data)\n",
    "\n",
    "def unlogify(data):\n",
    "    \"\"\"\n",
    "    Inverse transform to go from log-price to price.\n",
    "    \"\"\"\n",
    "    return np.exp(data) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(logify(y), bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will proceed with single-validation training\n",
    "\n",
    "\n",
    "### Training Stage\n",
    "1. we split data into train (5/6) and test (1/6) sets.\n",
    "2. train set will be split into 5 equal folds, and for each fold we will:\n",
    "    - fit a CatBoost categorical encoder\n",
    "    - encode test fold data using the encoder from step above\n",
    "    - train a model on the encoded data, optionally using other folds for hyperparameter tuning\n",
    "    \n",
    "### Testing Stage\n",
    "1. we put test data through encoders fit in the training stage\n",
    "2. for each encoded test data, we retrieve predictions for that model\n",
    "3. we combine predictions (average) to arrive at the final prediction\n",
    "4. evaluate error rate of the final prediction\n",
    "\n",
    "This can be summarized in the image below:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3224/1*JZ_42L2eO5YZ7rf7oNeCOQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the choise of categorical feature encoders\n",
    "\n",
    "I chose [CatBoost encoder](https://contrib.scikit-learn.org/categorical-encoding/catboost.html) because it encodes categorical features in one vector, as opposed to one-hot encoding, for example. This is particulary convenient when having multiple distinct features. If using one-hot encoding, this would inflate the number of variables and make it harder for the model to learn due to the curse of dimensionality. \n",
    "\n",
    "CatBoost uses information about relative frequencies of veatures and values of the target variable for the train data set to encode the features numerically in just one vector, which keeps the number of features unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test indices and validation fold indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducibility\n",
    "random_idxs = np.random.permutation(len(data))\n",
    "split_idxs = np.array_split(random_idxs, 6)\n",
    "test_idxs, folds_idxs = split_idxs[0], split_idxs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns to encode using CatBoost\n",
    "categorical=[\n",
    "    'brand',\n",
    "    'colors',\n",
    "    'manufacturer',\n",
    "    'prices.condition',\n",
    "    'prices.currency',\n",
    "    'prices.isSale',\n",
    "    'prices.merchant',\n",
    "    'prices.offer',\n",
    "    'prices.returnPolicy',\n",
    "    'prices.shipping',\n",
    "]\n",
    "\n",
    "# make folds of data\n",
    "y_folds = []\n",
    "y_logified = []\n",
    "X_folds = []\n",
    "for fold_idxs in folds_idxs:\n",
    "    X_fold, y_fold = X.iloc[fold_idxs], y.iloc[fold_idxs]\n",
    "    X_folds.append(X_fold)\n",
    "    y_folds.append(y_fold)\n",
    "    y_logified.append(logify(y_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make encoders and encoded data folds\n",
    "encoders = []\n",
    "X_folds_encoded = []\n",
    "for X_fold, y_fold in zip(X_folds, y_logified):\n",
    "    cat_encoder = ce.CatBoostEncoder(cols=categorical)\n",
    "    X_encoded = cat_encoder.fit_transform(X_fold, y_fold)\n",
    "    X_folds_encoded.append(X_encoded)\n",
    "    encoders.append(cat_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train models (no hyperparameter optimization)\n",
    "models = []\n",
    "for X_train, y_train in zip(X_folds_encoded, y_logified):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = X.iloc[test_idxs], y.iloc[test_idxs]\n",
    "\n",
    "# put test data through encoders\n",
    "X_test_encoded_folds = []\n",
    "for e in encoders:\n",
    "    X_test_encoded = e.transform(X_test)\n",
    "    X_test_encoded_folds.append(X_test_encoded)\n",
    "    \n",
    "# make predictions through each of the corresponding models\n",
    "preds = []\n",
    "for X_test, model in zip(X_test_encoded_folds, models):\n",
    "    pred = model.predict(X_test)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_aggregated = unlogify(np.array(preds).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, preds_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, the models were able to learn some relationship, but it by far not perfect.\n",
    "\n",
    "If we convert to log-space then we see that relationship looks much better.\n",
    "\n",
    "One reason for this is that I did not perform hyperparameter optimization due to time constraints.\n",
    "\n",
    "Another reason is the data set was very noisy, with many variables having a lot of NaNs. If I had more time, I could perform some ablation studies and examine whether removing some of the variables would improve the situation.\n",
    "\n",
    "Another way to make the model perform better is to put more work into some of the variables that I discarded, or try to exctract more information from existing variables. For example, an NLP model could be applied to product descriptions, or available sizes information could be parsed in a way that gives more information that just the number of sizes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds_aggregated, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(logify(preds_aggregated), logify(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see distributions of the log-price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(logify(preds_aggregated), bins=30, range=(0,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(logify(y_test), bins=30, range=(0,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
