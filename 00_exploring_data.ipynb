{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 0. First Encounter With the Data\n",
    "### By Max Pechyonkin\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "> <strong><font color=red>Note:</font></strong>\n",
    "I understand that you might have limited time. So, if you would rather read only one notebook, rather than all three, please only read the last one, called `02_data_pipeline_refactored.ipynb`, it has the final result. The first two notebooks contained information about how I arrived at that result.\n",
    "\n",
    "This is my first attempt of analyzing the data, in which I wasn't aware of any pitfalls lying in front of me.\n",
    "It has my first encounters with inconsistencies within the data and some of preliminary exploration, when I was desiding on approaches to take regarding each variable. Some of the information from this analysis went into the hand-written digital PDF notes accompanying these notebooks. I suggest reading the notes as they are more structured.\n",
    "\n",
    "## Exploring variables and what they mean\n",
    "\n",
    "Data comes from [this](https://query.data.world/s/jgwfznif4ngihwpk6sp5ghugpuqexb) link, and data schema is [here](https://developer.datafiniti.co/docs/product-data-schema).\n",
    "\n",
    "The data has a lot of features, so let's have a look at the columns first, and then dig into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '7004_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH) as f:\n",
    "    header = f.readline()\n",
    "\n",
    "col_names = header.strip().split(',')\n",
    "len(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = {\n",
    "# 'id': 'str',\n",
    "# 'asins': 'list-str',\n",
    "# 'brand': 'str',\n",
    "# 'categories': ,\n",
    "# 'colors',\n",
    "# 'count',\n",
    "# 'dateAdded',\n",
    "# 'dateUpdated',\n",
    "# 'descriptions',\n",
    "# 'dimension',\n",
    "# 'ean',\n",
    "# 'features',\n",
    "# 'flavors',\n",
    "# 'imageURLs',\n",
    "# 'isbn',\n",
    "# 'keys',\n",
    "# 'manufacturer',\n",
    "# 'manufacturerNumber',\n",
    "# 'merchants',\n",
    "# 'name',\n",
    "# 'prices.amountMin',\n",
    "# 'prices.amountMax',\n",
    "# 'prices.availability',\n",
    "# 'prices.color',\n",
    "# 'prices.condition',\n",
    "# 'prices.count',\n",
    "# 'prices.currency',\n",
    "# 'prices.dateAdded',\n",
    "# 'prices.dateSeen',\n",
    "# 'prices.flavor',\n",
    "# 'prices.isSale',\n",
    "# 'prices.merchant',\n",
    "# 'prices.offer',\n",
    "# 'prices.returnPolicy',\n",
    "# 'prices.shipping',\n",
    "# 'prices.size',\n",
    "# 'prices.source',\n",
    "# 'prices.sourceURLs',\n",
    "# 'prices.warranty',\n",
    "# 'quantities',\n",
    "# 'reviews',\n",
    "# 'sizes',\n",
    "# 'skus',\n",
    "# 'sourceURLs',\n",
    "# 'upc',\n",
    "# 'vin',\n",
    "# 'websiteIDs',\n",
    "# 'weight'\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the `csv` file provided for the assignment is not valid -- some of the rows have an incorrect number of fields. Let's ignore those rows, but print the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, error_bad_lines=False, warn_bad_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I copied the warnings into a Python string to count the number of bad rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'Skipping line 251: expected 48 fields, saw 49\\nSkipping line 444: expected 48 fields, saw 50\\nSkipping line 847: expected 48 fields, saw 49\\nSkipping line 848: expected 48 fields, saw 49\\nSkipping line 1018: expected 48 fields, saw 49\\nSkipping line 1575: expected 48 fields, saw 51\\nSkipping line 2133: expected 48 fields, saw 49\\nSkipping line 2922: expected 48 fields, saw 51\\nSkipping line 3777: expected 48 fields, saw 51\\nSkipping line 4057: expected 48 fields, saw 49\\nSkipping line 4239: expected 48 fields, saw 49\\nSkipping line 4240: expected 48 fields, saw 49\\nSkipping line 4384: expected 48 fields, saw 49\\nSkipping line 4385: expected 48 fields, saw 49\\nSkipping line 5388: expected 48 fields, saw 49\\nSkipping line 5480: expected 48 fields, saw 49\\nSkipping line 5481: expected 48 fields, saw 49\\nSkipping line 5907: expected 48 fields, saw 50\\nSkipping line 5908: expected 48 fields, saw 50\\nSkipping line 6600: expected 48 fields, saw 49\\nSkipping line 6601: expected 48 fields, saw 49\\nSkipping line 6602: expected 48 fields, saw 49\\nSkipping line 6603: expected 48 fields, saw 49\\nSkipping line 7227: expected 48 fields, saw 49\\nSkipping line 7228: expected 48 fields, saw 49\\nSkipping line 7229: expected 48 fields, saw 49\\nSkipping line 7267: expected 48 fields, saw 51\\nSkipping line 9025: expected 48 fields, saw 49\\nSkipping line 9026: expected 48 fields, saw 49\\nSkipping line 9027: expected 48 fields, saw 49\\nSkipping line 9417: expected 48 fields, saw 50\\nSkipping line 10815: expected 48 fields, saw 49\\nSkipping line 11034: expected 48 fields, saw 50\\nSkipping line 11059: expected 48 fields, saw 49\\nSkipping line 11060: expected 48 fields, saw 49\\nSkipping line 11339: expected 48 fields, saw 50\\nSkipping line 11357: expected 48 fields, saw 49\\nSkipping line 11358: expected 48 fields, saw 49\\nSkipping line 11646: expected 48 fields, saw 49\\nSkipping line 11647: expected 48 fields, saw 49\\nSkipping line 12161: expected 48 fields, saw 50\\nSkipping line 12307: expected 48 fields, saw 49\\nSkipping line 12308: expected 48 fields, saw 49\\nSkipping line 12614: expected 48 fields, saw 49\\nSkipping line 12615: expected 48 fields, saw 49\\nSkipping line 12616: expected 48 fields, saw 49\\nSkipping line 12617: expected 48 fields, saw 49\\nSkipping line 12618: expected 48 fields, saw 49\\nSkipping line 12619: expected 48 fields, saw 49\\nSkipping line 12620: expected 48 fields, saw 49\\nSkipping line 12621: expected 48 fields, saw 49\\nSkipping line 12622: expected 48 fields, saw 49\\nSkipping line 12623: expected 48 fields, saw 49\\nSkipping line 12799: expected 48 fields, saw 49\\nSkipping line 14200: expected 48 fields, saw 49\\nSkipping line 14595: expected 48 fields, saw 49\\n'\n",
    "b'Skipping line 16747: expected 48 fields, saw 49\\nSkipping line 16748: expected 48 fields, saw 49\\nSkipping line 16749: expected 48 fields, saw 49\\nSkipping line 16750: expected 48 fields, saw 49\\nSkipping line 16751: expected 48 fields, saw 49\\nSkipping line 16752: expected 48 fields, saw 49\\nSkipping line 16753: expected 48 fields, saw 49\\nSkipping line 17318: expected 48 fields, saw 49\\nSkipping line 17319: expected 48 fields, saw 49\\nSkipping line 17766: expected 48 fields, saw 49\\nSkipping line 17767: expected 48 fields, saw 49\\nSkipping line 18000: expected 48 fields, saw 52\\nSkipping line 18001: expected 48 fields, saw 52\\nSkipping line 18308: expected 48 fields, saw 51\\nSkipping line 19223: expected 48 fields, saw 52\\nSkipping line 19224: expected 48 fields, saw 52\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.count('Skipping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in total 56 rows were invalid in the data, so we skipped them.\n",
    "\n",
    "Now, we can have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the first 20 rows of the data frame, it seems that there are duplicates in `id` field of the data, at least in the visible columns. Let's inspect this hypothesis and if true, we will need to remove the duplicates.\n",
    "\n",
    "I will use vertical layout for better visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row0 = df.iloc[0]\n",
    "row1 = df.iloc[1]\n",
    "data1 = pd.DataFrame([row0, row1]).dropna(axis=1, how='all') # ignore labels that are NaNs in both rows\n",
    "data1 = pd.DataFrame([data1.iloc[0], data1.iloc[1], data1.iloc[0] == data1.iloc[1]])\n",
    "data1.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences are in columns:\n",
    "- `prices.isSale` - this probably determines the minimum and maximum prices to be different\n",
    "- `prices.amountMin`\n",
    "- `prices.amountMax`\n",
    "- `prices.condition` - NaN vs new\n",
    "- `prices.merchant` - NaN vs UnbeatableSale - Walmart.com\n",
    "\n",
    "OK, it looks like the same `id` can be in different rows depending on the merchant and sale.\n",
    "\n",
    "Let's look further into more examples, and pay attention to columns that were different in the first example.\n",
    "\n",
    "Let's grab the second unique `id` from the dataframe and have a look at difference in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 'AVpfHsWP1cnluZ0-eVZ7'\n",
    "data2 = df[df.id == sample_id].dropna(axis=1, how='all') # again, drop NaNs to save visual space\n",
    "data2.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1**: An interesting observation. This `id` is associated with [this](https://www.internationalsafety.com/products/overshoes-servus-studs-by-honeywell) product. There are not shoes per se, but rather overshoes that are worn in showy and icy weather that increase grip between shoes and surface. For now I will assume this is a valid product for the task I was given.\n",
    "\n",
    "**Note 2**: I wonder if there are irrelevant products in the data, that are not shoes for men. I will get to exploring this later.\n",
    "\n",
    "My interest at this point is to see for which columns there are differences.\n",
    "I will retreive column numbers that have more than 1 unique values and use them to only see those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_numbers = data2.nunique(axis=0)\n",
    "non_unique_cols = unique_numbers[unique_numbers > 1].index\n",
    "non_unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[non_unique_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second unique `id` that I examined, we can see the following three columns are similar to the example we saw before:\n",
    "- `prices.isSale`\n",
    "- `prices.amountMin`\n",
    "- `prices.amountMax`\n",
    "In addition, there are two more columns with differences:\n",
    "- `prices.dateSeen`\n",
    "- `prices.offer`\n",
    "\n",
    "My conclusion at this point regarding mupltiple same `id`s in the data is as follows (it also takes into account explanation in the PDF that came with this assignment):\n",
    "\n",
    "The same product `id` can be seen in the multiple rows in the data because there might be: \n",
    "- different vendors selling the product\n",
    "- different times the same vendor was selling this `id` at differenct prices\n",
    "- sales that also affect the price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NaNs\n",
    "\n",
    "First, let's have a look at the unqique values, sorted from largest to smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 5 rows are suspicious -- they must be NaNs. Let's have a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_cols = df.isnull().all()\n",
    "all_nan_cols[all_nan_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_col_names = all_nan_cols[all_nan_cols].index\n",
    "all_nan_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(all_nan_col_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, let's also see percentages of NaNs for each column. If some columns have too many NaNs, let's get rid of those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = df.shape\n",
    "nrows, ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nans_percentages(data):\n",
    "    nrows, _ = data.shape\n",
    "    return (data.isnull().sum(axis=0) / nrows).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_percentages_by_column = get_nans_percentages(df)\n",
    "nans_percentages_by_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One stragedy would be to remove some of the variables with missing data.\n",
    "At this point there are several possibilities to deal with this:\n",
    "\n",
    "- manually select some threshold for proportion of missing values and remove all columns that are above the threshold. This method is very easy but has drawbacks, e.g. why this particular threshold?\n",
    "- tread threshold for removing columns as a hyperparameter of the learning algorithm and select that using cross-validation\n",
    "- treat NaNs as one value of a categorical variable, because the NaN itself can be a signal to the model\n",
    "- use encoding that utilizes the knowledge about NaNs, such as CatBoost encoder\n",
    "\n",
    "For now, my approach is this:\n",
    "\n",
    "- remove all vars with more then 99% missing values\n",
    "- go through each of remaining vars and decide what transformation / parsing needs to be done\n",
    "- perform transformation or parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of columns with more than 99% missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_99_missing_cols = nans_percentages_by_column[nans_percentages_by_column > 0.99].index\n",
    "more_than_99_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(more_than_99_missing_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate percentages because we removed some columns\n",
    "nans_percentages_by_column = get_nans_percentages(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_non_nan_entries(colname: str, data: pd.DataFrame = df) -> pd.Series:\n",
    "    return data[colname][data[colname].isnull() == False]\n",
    "\n",
    "for cname in nans_percentages_by_column[nans_percentages_by_column < 0.01].index:\n",
    "    print(f\"COLUMN {cname}\")\n",
    "    print(check_non_nan_entries(cname))\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sourceURLs.iloc[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
