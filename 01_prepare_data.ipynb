{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1. Processing Pipeline Exploration\n",
    "### By Max Pechyonkin\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "> <strong><font color=red>Note:</font></strong>\n",
    "I understand that you might have limited time. So, if you would rather read only one notebook, rather than all three, please only read the last one, called `02_data_pipeline_refactored.ipynb`, it has the final result. The first two notebooks contained information about how I arrived at that result.\n",
    "\n",
    "This is an exploratory notebook, where I go step by step and figure out how to deal with this or that feature. While reading this notebook, please also look at the accompanying hand-written digital notes in the PDF format that I submitted with the project. They provide some clarity about why I made certain decisions. \n",
    "\n",
    "This is notebook is not production-ready. The next notebook performs refactoring and data processing in one simple API call: `datacleaner.transform(X)`, which uses all the steps that I devised while working on the data in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in CSV, ignore invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATA_PATH = '7004_1.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Remove rows with all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_cols = df.isnull().all()\n",
    "all_nan_cols[all_nan_cols]\n",
    "\n",
    "all_nan_col_names = all_nan_cols[all_nan_cols].index\n",
    "all_nan_col_names\n",
    "\n",
    "df = df.drop(all_nan_col_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Remove rows with NaNs percentage > 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nans_percentages(data):\n",
    "    \"\"\"\n",
    "    Get percentages of NaNs for each column.\n",
    "    \n",
    "    Returns Series with index of column names and float values.\n",
    "    \"\"\"\n",
    "    nrows, _ = data.shape\n",
    "    return (data.isnull().sum(axis=0) / nrows).sort_values(ascending=False)\n",
    "\n",
    "def get_colnames_with_nans_above_thresh(data, thresh: float):\n",
    "    \"\"\"\n",
    "    Return column names with NaNs proportions above the given level.\n",
    "    \"\"\"\n",
    "    percentages = get_nans_percentages(data)\n",
    "    result = percentages[percentages > thresh]\n",
    "    return result.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames_with_too_many_nans = get_colnames_with_nans_above_thresh(df, thresh=0.99)\n",
    "colnames_with_too_many_nans\n",
    "\n",
    "df = df.drop(colnames_with_too_many_nans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Remove marked columns ðŸ”´\n",
    "\n",
    "Analysis why columns below need to be removed is in the notes PDF accompanying this submission.\n",
    "\n",
    "Main reasons are:\n",
    "\n",
    "- features that are `id`-like, i.e. we don't want to train for a specific product id, this will overfit\n",
    "- features that I deemed not very relevant - such as dates, even though I could be possible to turn those dates into categorical features like day of the week, holiday, season etc. For now I decided to not put additional effort in there and focus on building a working MVP.\n",
    "- features having NLP data, such as reviews, name of the product. I could build an NLP feature exctractor based on a neural net that could probably work, but to save time I ignore it for now\n",
    "- URLs to images and webpages. Some of them are not available, or extracting requires additional work that is not feasible given my time constraints for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cname in df.columns:\n",
    "    print(f\"    '{cname}',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\n",
    "    'id',\n",
    "    'asins',\n",
    "    'dateAdded',\n",
    "    'dateUpdated',\n",
    "    'descriptions',\n",
    "    'ean',\n",
    "    'features',\n",
    "    'imageURLs',\n",
    "    'keys',\n",
    "    'manufacturerNumber',\n",
    "    'merchants',\n",
    "    'name',\n",
    "    'prices.amountMin',\n",
    "    'prices.dateAdded',\n",
    "    'prices.dateSeen',\n",
    "    'prices.sourceURLs',\n",
    "    'skus',\n",
    "    'sourceURLs',\n",
    "    'upc',\n",
    "    'weight',\n",
    "]\n",
    "\n",
    "df = df.drop(cols_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with currency and price conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.currency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First observation - there appear to be some illegal currencies. Second is that there are not only prices in USD, but also in AUD, CAD, EUR and GBP. I could discard that, but I will convert all prices to USD. The best way to do it would be to retrieve the exchange rates for when the prices were published, but I will simplify and take the current rates. In production, this would definitely require more careful analysis. \n",
    "\n",
    "Another question I would ask is whether features are good predictors in other markets that use other currencies. It might be the case that for a given local market it is better to have a separate model that learn important relationships in the local market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currency, Part 1: Remove rows that are not currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currencies_all = df['prices.currency'].value_counts()\n",
    "currencies_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_currencies = currencies_all[currencies_all > 20 ].index\n",
    "legal_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'SHOEBACCA LTD. - Walmart.com' not in legal_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_illegal_currency = df['prices.currency'].apply(lambda x: x not in legal_currencies)\n",
    "rows_illegal_fx = rows_illegal_currency[rows_illegal_currency == True].index\n",
    "rows_illegal_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(rows_illegal_fx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.currency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currency, Part 2: calculate the price based on currency and FX rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `fx_rates` is a mapping from dictionary to a scalar that will be multiply price in currency to make it price in dollars\n",
    "- I used Google to get the rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_rates = {\n",
    "    'USD': 1.00,\n",
    "    'AUD': 0.68,\n",
    "    'CAD': 0.75,\n",
    "    'EUR': 1.10,\n",
    "    'GBP': 1.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.amountMax'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.currency'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_invalid_price_string(s):\n",
    "    s = str(s)\n",
    "    illegal_chars = '-T:Z'\n",
    "    for c in illegal_chars:\n",
    "        if c in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "illegal_price_mask = df['prices.amountMax'].apply(is_invalid_price_string)\n",
    "illegal_price_idxs = illegal_price_mask[illegal_price_mask].index\n",
    "illegal_price_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, these rows clearly don't belong here. Not only the price is illegal, it looks like these are not even shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[illegal_price_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(illegal_price_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_usd(row):\n",
    "    price = float(row['prices.amountMax'])\n",
    "    currency = row['prices.currency']\n",
    "    if not currency:\n",
    "        # if currency is NaN assume USD and return price\n",
    "        return price\n",
    "    fx_rate = fx_rates[currency]\n",
    "    return price * fx_rate\n",
    "\n",
    "df['price'] = df.apply(convert_to_usd, axis=1)\n",
    "df = df.drop(['prices.amountMax'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `isSale` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_nans(colname, data, display=True):\n",
    "    uniques = data[colname].unique()\n",
    "    nans = data[colname].isnull().sum()\n",
    "    print(uniques)\n",
    "    print(f'Uniques: {len(uniques)}')\n",
    "    print(f'Nans: {nans}')\n",
    "    if not display:\n",
    "        return uniques, nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.isSale', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_is_sale(x):\n",
    "    if isinstance(x, bool):\n",
    "        return x\n",
    "    elif x.capitalize() == 'True':\n",
    "        return True\n",
    "    elif x.capitalize() == 'False':\n",
    "        return False\n",
    "    else:\n",
    "        print(x)\n",
    "        print(type(x))\n",
    "        raise ValueError(\"Something went wrong!\")\n",
    "\n",
    "df['prices.isSale'] = df['prices.isSale'].apply(process_is_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.isSale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `categories` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since categories are presented as comma separated values, let's see how many unique categories are there by splitting them.\n",
    "\n",
    "After splitting, I realized that the categoriez are not very informative and decided to remove the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "\n",
    "for string in df.categories.unique():\n",
    "    string = string.lower()\n",
    "    if 'shoes' not in string:\n",
    "        print(string)\n",
    "    subcategories = string.split(',')\n",
    "    subcategories = [s.strip() for s in subcategories]\n",
    "    for s in subcategories:\n",
    "        if s not in all_categories:\n",
    "            all_categories.append(s)\n",
    "            \n",
    "print(len(all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['categories'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `brand` column\n",
    "\n",
    "This one is ready to be categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('brand', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.merchant`\n",
    "\n",
    "Ready for categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.merchant', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.condition` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.condition', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `colors` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly complex column, with a lot of messy colors. I will simply make it lower-case and remove dashes for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_colors, nans_color = get_unique_nans('colors', df, display=False)\n",
    "unique_colors, nans_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['colors'] = df['colors'].apply(lambda x: x.lower().replace(' ', '').replace('-', '') if not pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `manufacturer` column\n",
    "This one is ready for encoding as a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('manufacturer', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.offer` column\n",
    "\n",
    "I will leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.offer', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.shipping` column\n",
    "\n",
    "I will broadly categorize this variable into \"Free\" vs \"Not Free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.shipping', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shipping(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    \n",
    "    x = x.lower()\n",
    "    if 'free' in x:\n",
    "        return 'free'\n",
    "    else:\n",
    "        return 'not free'\n",
    "    return x\n",
    "    \n",
    "    \n",
    "df['prices.shipping'] = df['prices.shipping'].apply(process_shipping)\n",
    "df['prices.shipping'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `sizes` column\n",
    "\n",
    "To simplify, I will calculate how many sizes are available for a particulate shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('sizes', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sizes(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    return len(x.split(','))\n",
    "\n",
    "df['sizes'] = df['sizes'].apply(process_sizes)\n",
    "df['sizes'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `dimension` column\n",
    "\n",
    "Let's see if dimension can be easily processed.\n",
    "\n",
    "To simplify, I will create a dimension number as the sum of all 3 dimensions. This will help to deal with inconsistencies of the order of the three dimensions (e.g. is it height-width-depth or depth-heitht-width or some other combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dims, _ = get_unique_nans('dimension', df, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in unique_dims:\n",
    "    if not pd.isnull(d) and len(d.split('x')) != 3:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means almost all dimensions can be split by `x` except for '32 inches' one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimensions(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    \n",
    "    # deal with edge case\n",
    "    if x == '32 inches':\n",
    "        return 32.0\n",
    "    \n",
    "    x = x.lower()\n",
    "    dims = x.split('x')\n",
    "    dims = [d.replace(' ', '').replace('in', '') for d in dims]\n",
    "    dims = map(float, dims)\n",
    "    return sum(dims)\n",
    "    \n",
    "    \n",
    "df['dimension'] = df['dimension'].apply(process_dimensions)\n",
    "df['dimension']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.size` column\n",
    "\n",
    "Drop this column, since too few values and mostly very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.size', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('prices.size', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.color` column\n",
    "\n",
    "Drop this too, for same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.color', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('prices.color', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning `prices.returnPolicy` column\n",
    "\n",
    "Ready for categorizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_nans('prices.returnPolicy', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices.returnPolicy'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Average Review Score\n",
    "\n",
    "I decided to calculate average rating for reviews with rating. If there is no rating, return zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reviews_idxs = df['reviews'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews'][~no_reviews_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "\n",
    "    rating = 0.0\n",
    "    try:\n",
    "        reviews = json.loads(x)\n",
    "        for r in reviews:\n",
    "            if 'rating' in r.keys():\n",
    "                rating += float(r['rating'])\n",
    "        return rating / len(reviews)\n",
    "    except:\n",
    "        return rating\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(parse_review)\n",
    "df['reviews'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of price outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.price > 2000].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('price', axis=1), df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical=[\n",
    "    'brand',\n",
    "    'colors',\n",
    "    'manufacturer',\n",
    "    'prices.condition',\n",
    "    'prices.currency',\n",
    "    'prices.isSale',\n",
    "    'prices.merchant',\n",
    "    'prices.offer',\n",
    "    'prices.returnPolicy',\n",
    "    'prices.shipping',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply random permutation to the data (required by CatBoost)\n",
    "perm = np.random.permutation(len(X))\n",
    "X = X.iloc[perm].reset_index(drop=True)\n",
    "y = y.iloc[perm].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_cat = ce.CatBoostEncoder(cols=categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ce_cat.fit_transform(X_train, y_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logify(data):\n",
    "    return np.log(1 + data)\n",
    "\n",
    "def unlogify(data):\n",
    "    return np.exp(data) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = logify(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(max_leaf_nodes=None, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = ce_cat.transform(X_test)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill_nas = ['dimension', 'reviews', 'sizes']\n",
    "for c in cols_to_fill_nas:\n",
    "    X_test_transformed[c] = X_test_transformed[c].fillna(-999.9)\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(preds, logify(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds, logify(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(unlogify(preds), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(1+y_train), bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = np.log(1+y_train)\n",
    "un = np.exp(lg) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(un, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
